{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from nltk.corpus import stopwords\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "\n",
    "# Read the CSV file into a DataFrame: df\n",
    "df = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
    "# Sample 10% of dataset\n",
    "df = df.sample(frac=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41384, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394349</th>\n",
       "      <td>Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244.95</td>\n",
       "      <td>5</td>\n",
       "      <td>Very good one! Better than Samsung S and iphon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34377</th>\n",
       "      <td>Apple iPhone 5c 8GB (Pink) - Verizon Wireless</td>\n",
       "      <td>Apple</td>\n",
       "      <td>194.99</td>\n",
       "      <td>1</td>\n",
       "      <td>The phone needed a SIM card, would have been n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248521</th>\n",
       "      <td>Motorola Droid RAZR MAXX XT912 M Verizon Smart...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>174.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I was 3 months away from my upgrade and my Str...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167661</th>\n",
       "      <td>CNPGD [U.S. Office Extended Warranty] Smartwat...</td>\n",
       "      <td>CNPGD</td>\n",
       "      <td>49.99</td>\n",
       "      <td>1</td>\n",
       "      <td>an experience i want to forget</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73287</th>\n",
       "      <td>Apple iPhone 7 Unlocked Phone 256 GB - US Vers...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>922.00</td>\n",
       "      <td>5</td>\n",
       "      <td>GREAT PHONE WORK ACCORDING MY EXPECTATIONS.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name Brand Name   Price  \\\n",
       "394349  Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...        NaN  244.95   \n",
       "34377       Apple iPhone 5c 8GB (Pink) - Verizon Wireless      Apple  194.99   \n",
       "248521  Motorola Droid RAZR MAXX XT912 M Verizon Smart...   Motorola  174.99   \n",
       "167661  CNPGD [U.S. Office Extended Warranty] Smartwat...      CNPGD   49.99   \n",
       "73287   Apple iPhone 7 Unlocked Phone 256 GB - US Vers...      Apple  922.00   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "394349       5  Very good one! Better than Samsung S and iphon...   \n",
       "34377        1  The phone needed a SIM card, would have been n...   \n",
       "248521       5  I was 3 months away from my upgrade and my Str...   \n",
       "167661       1                     an experience i want to forget   \n",
       "73287        5        GREAT PHONE WORK ACCORDING MY EXPECTATIONS.   \n",
       "\n",
       "        Review Votes  \n",
       "394349           0.0  \n",
       "34377            1.0  \n",
       "248521           3.0  \n",
       "167661           0.0  \n",
       "73287            1.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[pd.notnull(df['Reviews'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(r'[/(){}\\[\\]\\|@,;.#+_]',' ', text) \n",
    "    text = re.sub(r'[^0-9a-z ]','', text) \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reviews'] = df['Reviews'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394349</th>\n",
       "      <td>Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244.95</td>\n",
       "      <td>5</td>\n",
       "      <td>good one better samsung iphones quality camera...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34377</th>\n",
       "      <td>Apple iPhone 5c 8GB (Pink) - Verizon Wireless</td>\n",
       "      <td>Apple</td>\n",
       "      <td>194.99</td>\n",
       "      <td>1</td>\n",
       "      <td>phone needed sim card would nice know</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248521</th>\n",
       "      <td>Motorola Droid RAZR MAXX XT912 M Verizon Smart...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>174.99</td>\n",
       "      <td>5</td>\n",
       "      <td>3 months away upgrade stratosphere kept crappi...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167661</th>\n",
       "      <td>CNPGD [U.S. Office Extended Warranty] Smartwat...</td>\n",
       "      <td>CNPGD</td>\n",
       "      <td>49.99</td>\n",
       "      <td>1</td>\n",
       "      <td>experience want forget</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73287</th>\n",
       "      <td>Apple iPhone 7 Unlocked Phone 256 GB - US Vers...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>922.00</td>\n",
       "      <td>5</td>\n",
       "      <td>great phone work according expectations</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name Brand Name   Price  \\\n",
       "394349  Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...        NaN  244.95   \n",
       "34377       Apple iPhone 5c 8GB (Pink) - Verizon Wireless      Apple  194.99   \n",
       "248521  Motorola Droid RAZR MAXX XT912 M Verizon Smart...   Motorola  174.99   \n",
       "167661  CNPGD [U.S. Office Extended Warranty] Smartwat...      CNPGD   49.99   \n",
       "73287   Apple iPhone 7 Unlocked Phone 256 GB - US Vers...      Apple  922.00   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "394349       5  good one better samsung iphones quality camera...   \n",
       "34377        1              phone needed sim card would nice know   \n",
       "248521       5  3 months away upgrade stratosphere kept crappi...   \n",
       "167661       1                             experience want forget   \n",
       "73287        5            great phone work according expectations   \n",
       "\n",
       "        Review Votes  \n",
       "394349           0.0  \n",
       "34377            1.0  \n",
       "248521           3.0  \n",
       "167661           0.0  \n",
       "73287            1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41374 entries, 394349 to 109303\n",
      "Data columns (total 6 columns):\n",
      "Product Name    41374 non-null object\n",
      "Brand Name      34837 non-null object\n",
      "Price           40753 non-null float64\n",
      "Rating          41374 non-null int64\n",
      "Reviews         41374 non-null object\n",
      "Review Votes    40184 non-null float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40753.000000</td>\n",
       "      <td>41374.000000</td>\n",
       "      <td>40184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>227.631428</td>\n",
       "      <td>3.815319</td>\n",
       "      <td>1.496740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>277.011277</td>\n",
       "      <td>1.551319</td>\n",
       "      <td>8.451689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.730000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>79.950000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>269.990000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2408.730000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>524.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price        Rating  Review Votes\n",
       "count  40753.000000  41374.000000  40184.000000\n",
       "mean     227.631428      3.815319      1.496740\n",
       "std      277.011277      1.551319      8.451689\n",
       "min        1.730000      1.000000      0.000000\n",
       "25%       79.950000      3.000000      0.000000\n",
       "50%      140.000000      5.000000      0.000000\n",
       "75%      269.990000      5.000000      1.000000\n",
       "max     2408.730000      5.000000    524.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Samsung       6539\n",
       "BLU           6295\n",
       "Apple         5623\n",
       "LG            2272\n",
       "BlackBerry    1699\n",
       "Name: Brand Name, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Brand Name'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) # drop any rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming rating with 3 are neutral reviews\n",
    "# so drop rows with rating = 3 (by chosing all the rows with rating!=3)\n",
    "\n",
    "df = df[df['Rating']!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming rating with greater than 3 are rated as postive\n",
    "# so we assign 1 to Positively rated and 0 to those are not\n",
    "# if Rating > 3, then 'Positively Rated' = 1, else 'Positively Rated' = 0\n",
    "\n",
    "df['Positively Rated'] = np.where(df['Rating']>3, 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34377</th>\n",
       "      <td>Apple iPhone 5c 8GB (Pink) - Verizon Wireless</td>\n",
       "      <td>Apple</td>\n",
       "      <td>194.99</td>\n",
       "      <td>1</td>\n",
       "      <td>phone needed sim card would nice know</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248521</th>\n",
       "      <td>Motorola Droid RAZR MAXX XT912 M Verizon Smart...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>174.99</td>\n",
       "      <td>5</td>\n",
       "      <td>3 months away upgrade stratosphere kept crappi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167661</th>\n",
       "      <td>CNPGD [U.S. Office Extended Warranty] Smartwat...</td>\n",
       "      <td>CNPGD</td>\n",
       "      <td>49.99</td>\n",
       "      <td>1</td>\n",
       "      <td>experience want forget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73287</th>\n",
       "      <td>Apple iPhone 7 Unlocked Phone 256 GB - US Vers...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>922.00</td>\n",
       "      <td>5</td>\n",
       "      <td>great phone work according expectations</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277158</th>\n",
       "      <td>Nokia N8 Unlocked GSM Touch Screen Phone Featu...</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>95.00</td>\n",
       "      <td>5</td>\n",
       "      <td>fell love phone everything suppose 3g network ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name Brand Name   Price  \\\n",
       "34377       Apple iPhone 5c 8GB (Pink) - Verizon Wireless      Apple  194.99   \n",
       "248521  Motorola Droid RAZR MAXX XT912 M Verizon Smart...   Motorola  174.99   \n",
       "167661  CNPGD [U.S. Office Extended Warranty] Smartwat...      CNPGD   49.99   \n",
       "73287   Apple iPhone 7 Unlocked Phone 256 GB - US Vers...      Apple  922.00   \n",
       "277158  Nokia N8 Unlocked GSM Touch Screen Phone Featu...      Nokia   95.00   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "34377        1              phone needed sim card would nice know   \n",
       "248521       5  3 months away upgrade stratosphere kept crappi...   \n",
       "167661       1                             experience want forget   \n",
       "73287        5            great phone work according expectations   \n",
       "277158       5  fell love phone everything suppose 3g network ...   \n",
       "\n",
       "        Review Votes  Positively Rated  \n",
       "34377            1.0                 0  \n",
       "248521           3.0                 1  \n",
       "167661           0.0                 0  \n",
       "73287            1.0                 1  \n",
       "277158           0.0                 1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7471776686078667"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Positively Rated'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],\n",
    "                                                    df['Positively Rated'],\n",
    "                                                test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24589,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to convert text into a numeric so that scikit-learn can use\n",
    "# The bag-of-words approach ignores structure and only counts how often each word occurs\n",
    "# CountVectorizer use the bag-of-words by converting text into a matrix of token counts.\n",
    "# First, we instantiate the CountVectorizer and fit it to our training data.\n",
    "\n",
    "# Fitting the CountVectorizer consists of the \n",
    "#     tokenization of the trained data and \n",
    "#     building of the vocabulary\n",
    "\n",
    "# Fitting the CountVectorizer \n",
    "#     tokenizes each document by finding \n",
    "#         all sequences of characters of \n",
    "#             at least two letters or \n",
    "#             numbers separated by word boundaries. \n",
    "# Converts everything to \n",
    "#     lowercase and \n",
    "#     builds a vocabulary using these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21907"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '000000',\n",
       " '000mah',\n",
       " '002order',\n",
       " '00k',\n",
       " '00us',\n",
       " '01',\n",
       " '0100']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', 'cdblackberry', 'fineoverall', 'mayor', 'reen', 'trim']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use transform method to transform X_train to a document term matrix\n",
    "# giving us the bag-of-word representation of X_train\n",
    "\n",
    "# This representation is stored in a SciPy sparse matrix where \n",
    "#     each row corresponds to a document and \n",
    "#     each column a word from our training vocabulary.\n",
    "\n",
    "# The entries in this matrix are the number of times each word appears in each document.\n",
    "\n",
    "# Because the number of words in the vocabulary is so much larger \n",
    "# than the number of words that might appear in a single review, \n",
    "# most entries of this matrix are zero.\n",
    "\n",
    "# and the shape will be \n",
    "#     number of document/rows(here in dataframe)/reviews(in this case) *\n",
    "#     number of words in the vocabulary/tokens\n",
    "\n",
    "# Here's a trivial example ... Let's suppose we have 3 documents:\n",
    "\n",
    "#     Doc1: Hello, World, the sun is shining\n",
    "#     Doc2: Hello world, the weather is nice\n",
    "#     Doc3: Hello world, the wind is cold\n",
    "\n",
    "\n",
    "# Then, our vocabulary would look like this (using 1-grams without stop word removal):\n",
    "\n",
    "#     Vocabulary: [hello, world, the, wind, weather, sun, is, shining, nice, cold]\n",
    "\n",
    "\n",
    "# The corresponding, binary feature vectors are:\n",
    "\n",
    "#     Doc1: [1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
    "#     Doc2: [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n",
    "#     Doc3: [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "\n",
    "# Which we use to construct the dense matrix / document term matrix:\n",
    "\n",
    "#     [[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
    "#      [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n",
    "#      [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24589x21907 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 431916 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24589, 21907)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression()\n",
    "#model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.959506 using {'penalty': 'l2', 'C': 0.4393970560760795}\n",
      "0.898060 (0.010759) with: {'penalty': 'l2', 'C': 163789.3706954068}\n",
      "0.872224 (0.012931) with: {'penalty': 'l1', 'C': 1389495.494373136}\n",
      "0.884193 (0.010541) with: {'penalty': 'l1', 'C': 19306.977288832535}\n",
      "0.500000 (0.000000) with: {'penalty': 'l1', 'C': 8.483428982440725e-05}\n",
      "0.954771 (0.002731) with: {'penalty': 'l1', 'C': 0.4393970560760795}\n",
      "0.959506 (0.002424) with: {'penalty': 'l2', 'C': 0.4393970560760795}\n",
      "0.926465 (0.004603) with: {'penalty': 'l2', 'C': 268.2695795279727}\n",
      "0.931290 (0.005036) with: {'penalty': 'l1', 'C': 31.622776601683793}\n",
      "0.869153 (0.011385) with: {'penalty': 'l1', 'C': 100000000.0}\n",
      "0.954474 (0.003170) with: {'penalty': 'l2', 'C': 0.05179474679231213}\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression Algorithm tuning\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "model = LogisticRegression()\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "scoring = 'roc_auc'\n",
    "logreg_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=scoring, n_jobs=-1, cv=kfold)\n",
    "logresult=logreg_cv.fit(X_train_vectorized, y_train)\n",
    "print(\"Best: %f using %s\" % (logresult.best_score_, logresult.best_params_))\n",
    "means = logresult.cv_results_['mean_test_score']\n",
    "stds = logresult.cv_results_['std_test_score']\n",
    "params = logresult.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9603940947754046\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(logreg_cv.score(vect.transform(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.80      0.84      1536\n",
      "          1       0.94      0.96      0.95      4612\n",
      "\n",
      "avg / total       0.92      0.92      0.92      6148\n",
      "\n",
      "Tuned Model Parameters: {'penalty': 'l2', 'C': 0.4393970560760795}\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test set: y_pred using Logistic Regression\n",
    "y_pred5 = logreg_cv.predict(vect.transform(X_test))\n",
    "# Compute and print metrics\n",
    "\n",
    "print(classification_report(y_test, y_pred5))\n",
    "print(\"Tuned Model Parameters: {}\".format(logreg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8819604428664354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomial NaÏve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.946520 using {'alpha': 0.4}\n",
      "0.847892 (0.002268) with: {'alpha': 0.0}\n",
      "0.942690 (0.002308) with: {'alpha': 0.1}\n",
      "0.945221 (0.002068) with: {'alpha': 0.2}\n",
      "0.946190 (0.002043) with: {'alpha': 0.30000000000000004}\n",
      "0.946520 (0.002114) with: {'alpha': 0.4}\n",
      "0.946520 (0.002180) with: {'alpha': 0.5}\n",
      "0.946335 (0.002338) with: {'alpha': 0.6000000000000001}\n",
      "0.946028 (0.002462) with: {'alpha': 0.7000000000000001}\n",
      "0.945647 (0.002532) with: {'alpha': 0.8}\n",
      "0.945098 (0.002626) with: {'alpha': 0.9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# MultinomialNB Algorithm tuning\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "model = MultinomialNB()\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "param_grid = {'alpha': alphas}\n",
    "scoring = 'roc_auc'\n",
    "nb_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=scoring, n_jobs=-1, cv=kfold)\n",
    "nbresult=nb_cv.fit(X_train_vectorized, y_train)\n",
    "print(\"Best: %f using %s\" % (nbresult.best_score_, nbresult.best_params_))\n",
    "means = nbresult.cv_results_['mean_test_score']\n",
    "stds = nbresult.cv_results_['std_test_score']\n",
    "params = nbresult.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9450336333884432\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.80      0.83      1536\n",
      "          1       0.93      0.96      0.94      4612\n",
      "\n",
      "avg / total       0.91      0.92      0.91      6148\n",
      "\n",
      "Tuned Model Parameters: {'alpha': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test set: y_pred using Logistic Regression\n",
    "y_pred5 = nb_cv.predict(vect.transform(X_test))\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(nb_cv.score(vect.transform(X_test), y_test)))\n",
    "print(classification_report(y_test, y_pred5))\n",
    "print(\"Tuned Model Parameters: {}\".format(nb_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8761044557675629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf–idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf–idf, or Term frequency-inverse document frequency\n",
    "# allows us to weight terms based on how important they are to a document.\n",
    "# high weight is given to terms that appear often in a particular document, \n",
    "# but don't appear often in the corpus. \n",
    "\n",
    "# Features with low tf–idf are either commonly used across all documents \n",
    "# or rarely used and only occur in long documents.\n",
    "\n",
    "# Features with high tf–idf are frequently used within specific documents, \n",
    "# but rarely used across all documents.\n",
    "# Similar to how we used CountVectorizer, \n",
    "# we'll instantiate the tf–idf vectorizer and fit it to our training data.\n",
    "\n",
    "# mindf, which allows us to specify a minimum number of documents \n",
    "# in which a token needs to appear to become part of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5577"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.956385 using {'penalty': 'l2', 'C': 0.05179474679231213}\n",
      "0.933282 (0.003799) with: {'penalty': 'l2', 'C': 2275.845926074791}\n",
      "0.887157 (0.008853) with: {'penalty': 'l1', 'C': 2275.845926074791}\n",
      "0.886106 (0.018924) with: {'penalty': 'l2', 'C': 11787686.347935867}\n",
      "0.875220 (0.011438) with: {'penalty': 'l2', 'C': 100000000.0}\n",
      "0.500000 (0.000000) with: {'penalty': 'l1', 'C': 0.0007196856730011522}\n",
      "0.858738 (0.002169) with: {'penalty': 'l2', 'C': 8.483428982440725e-05}\n",
      "0.500000 (0.000000) with: {'penalty': 'l1', 'C': 8.483428982440725e-05}\n",
      "0.956385 (0.002599) with: {'penalty': 'l2', 'C': 0.05179474679231213}\n",
      "0.866194 (0.008350) with: {'penalty': 'l1', 'C': 19306.977288832535}\n",
      "0.912671 (0.007466) with: {'penalty': 'l2', 'C': 19306.977288832535}\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression Algorithm tuning\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "model = LogisticRegression()\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "scoring = 'roc_auc'\n",
    "logreg_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=scoring, n_jobs=-1, cv=kfold)\n",
    "logresult=logreg_cv.fit(X_train_vectorized, y_train)\n",
    "print(\"Best: %f using %s\" % (logresult.best_score_, logresult.best_params_))\n",
    "means = logresult.cv_results_['mean_test_score']\n",
    "stds = logresult.cv_results_['std_test_score']\n",
    "params = logresult.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9553116784339766\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(logreg_cv.score(vect.transform(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.46      0.61      1536\n",
      "          1       0.85      0.99      0.91      4612\n",
      "\n",
      "avg / total       0.87      0.86      0.84      6148\n",
      "\n",
      "Tuned Model Parameters: {'penalty': 'l2', 'C': 0.05179474679231213}\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test set: y_pred using Logistic Regression\n",
    "y_pred5 = logreg_cv.predict(vect.transform(X_test))\n",
    "# Compute and print metrics\n",
    "\n",
    "print(classification_report(y_test, y_pred5))\n",
    "print(\"Tuned Model Parameters: {}\".format(logreg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7230941362207286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.964037 using {'alpha': 0.4}\n",
      "0.911889 (0.007580) with: {'alpha': 0.0}\n",
      "0.963279 (0.001009) with: {'alpha': 0.1}\n",
      "0.963804 (0.000965) with: {'alpha': 0.2}\n",
      "0.964006 (0.000928) with: {'alpha': 0.30000000000000004}\n",
      "0.964037 (0.000936) with: {'alpha': 0.4}\n",
      "0.964028 (0.000924) with: {'alpha': 0.5}\n",
      "0.963994 (0.000890) with: {'alpha': 0.6000000000000001}\n",
      "0.963994 (0.000878) with: {'alpha': 0.7000000000000001}\n",
      "0.963989 (0.000834) with: {'alpha': 0.8}\n",
      "0.963993 (0.000813) with: {'alpha': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB Algorithm tuning\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "model = MultinomialNB()\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "param_grid = {'alpha': alphas}\n",
    "scoring = 'roc_auc'\n",
    "nb_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=scoring, n_jobs=-1, cv=kfold)\n",
    "nbresult=nb_cv.fit(X_train_vectorized, y_train)\n",
    "print(\"Best: %f using %s\" % (nbresult.best_score_, nbresult.best_params_))\n",
    "means = nbresult.cv_results_['mean_test_score']\n",
    "stds = nbresult.cv_results_['std_test_score']\n",
    "params = nbresult.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9629978379544305\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.72      0.80      1536\n",
      "          1       0.91      0.97      0.94      4612\n",
      "\n",
      "avg / total       0.91      0.91      0.90      6148\n",
      "\n",
      "Tuned Model Parameters: {'alpha': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test set: y_pred using Logistic Regression\n",
    "y_pred5 = nb_cv.predict(vect.transform(X_test))\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(nb_cv.score(vect.transform(X_test), y_test)))\n",
    "print(classification_report(y_test, y_pred5))\n",
    "print(\"Tuned Model Parameters: {}\".format(nb_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8470184776127494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer+n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way we can add some context is by adding sequences of word features known as n-grams. \n",
    "\n",
    "# For example, bigrams, which count pairs of adjacent words, \n",
    "# could give us features such as is working versus not working. \n",
    "# And trigrams, which give us triplets of adjacent words, \n",
    "# could give us features such as not an issue.\n",
    "\n",
    "# To create these n-gram features, \n",
    "# we'll pass in a tuple to the parameter ngram_range, \n",
    "# where the values correspond to the minimum length and maximum lengths of sequences.\n",
    "\n",
    "# For example, if I pass in the tuple, 1, 2, \n",
    "# CountVectorizer will create features using the individual words, \n",
    "# as well as the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17594"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.965729 using {'penalty': 'l2', 'C': 268.2695795279727}\n",
      "0.941318 (0.005657) with: {'penalty': 'l1', 'C': 163789.3706954068}\n",
      "0.954273 (0.003934) with: {'penalty': 'l1', 'C': 268.2695795279727}\n",
      "0.929916 (0.006720) with: {'penalty': 'l1', 'C': 11787686.347935867}\n",
      "0.955284 (0.002589) with: {'penalty': 'l2', 'C': 100000000.0}\n",
      "0.965729 (0.002511) with: {'penalty': 'l2', 'C': 268.2695795279727}\n",
      "0.879088 (0.002774) with: {'penalty': 'l2', 'C': 8.483428982440725e-05}\n",
      "0.935931 (0.007165) with: {'penalty': 'l1', 'C': 1389495.494373136}\n",
      "0.500000 (0.000000) with: {'penalty': 'l1', 'C': 0.0007196856730011522}\n",
      "0.960535 (0.003025) with: {'penalty': 'l2', 'C': 2275.845926074791}\n",
      "0.500000 (0.000000) with: {'penalty': 'l1', 'C': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression Algorithm tuning\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "model = LogisticRegression()\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "scoring = 'roc_auc'\n",
    "logreg_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=scoring, n_jobs=-1, cv=kfold)\n",
    "logresult=logreg_cv.fit(X_train_vectorized, y_train)\n",
    "print(\"Best: %f using %s\" % (logresult.best_score_, logresult.best_params_))\n",
    "means = logresult.cv_results_['mean_test_score']\n",
    "stds = logresult.cv_results_['std_test_score']\n",
    "params = logresult.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966251493499747\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(logreg_cv.score(vect.transform(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.84      0.85      1536\n",
      "          1       0.95      0.95      0.95      4612\n",
      "\n",
      "avg / total       0.92      0.93      0.92      6148\n",
      "\n",
      "Tuned Model Parameters: {'penalty': 'l2', 'C': 268.2695795279727}\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test set: y_pred using Logistic Regression\n",
    "y_pred5 = logreg_cv.predict(vect.transform(X_test))\n",
    "# Compute and print metrics\n",
    "\n",
    "print(classification_report(y_test, y_pred5))\n",
    "print(\"Tuned Model Parameters: {}\".format(logreg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.896070204087164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.970141 using {'alpha': 0.1}\n",
      "0.903113 (0.003296) with: {'alpha': 0.0}\n",
      "0.970141 (0.000443) with: {'alpha': 0.1}\n",
      "0.970069 (0.000578) with: {'alpha': 0.2}\n",
      "0.969918 (0.000617) with: {'alpha': 0.30000000000000004}\n",
      "0.969764 (0.000666) with: {'alpha': 0.4}\n",
      "0.969693 (0.000708) with: {'alpha': 0.5}\n",
      "0.969630 (0.000723) with: {'alpha': 0.6000000000000001}\n",
      "0.969615 (0.000765) with: {'alpha': 0.7000000000000001}\n",
      "0.969634 (0.000793) with: {'alpha': 0.8}\n",
      "0.969644 (0.000818) with: {'alpha': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB Algorithm tuning\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "model = MultinomialNB()\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "param_grid = {'alpha': alphas}\n",
    "scoring = 'roc_auc'\n",
    "nb_cv = RandomizedSearchCV(estimator=model, param_distributions=param_grid, scoring=scoring, n_jobs=-1, cv=kfold)\n",
    "nbresult=nb_cv.fit(X_train_vectorized, y_train)\n",
    "print(\"Best: %f using %s\" % (nbresult.best_score_, nbresult.best_params_))\n",
    "means = nbresult.cv_results_['mean_test_score']\n",
    "stds = nbresult.cv_results_['std_test_score']\n",
    "params = nbresult.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9694993613806374\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.80      0.84      1536\n",
      "          1       0.93      0.97      0.95      4612\n",
      "\n",
      "avg / total       0.92      0.92      0.92      6148\n",
      "\n",
      "Tuned Model Parameters: {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels of the test set: y_pred using Logistic Regression\n",
    "y_pred5 = nb_cv.predict(vect.transform(X_test))\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(nb_cv.score(vect.transform(X_test), y_test)))\n",
    "print(classification_report(y_test, y_pred5))\n",
    "print(\"Tuned Model Parameters: {}\".format(nb_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8817424878939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
